{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai,os,sys\n",
    "from time import sleep, time\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "from dspy.evaluate import Evaluate\n",
    "from dspy.teleprompt import MIPRO\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>year</th>\n",
       "      <th>label</th>\n",
       "      <th>orig_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>687</td>\n",
       "      <td>Setting the horizon on the interest rate caps ...</td>\n",
       "      <td>2006</td>\n",
       "      <td>2</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>595</td>\n",
       "      <td>Nonetheless, employment is still 9.5 million b...</td>\n",
       "      <td>1999</td>\n",
       "      <td>0</td>\n",
       "      <td>576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>824</td>\n",
       "      <td>The shifting balance of domestic demand and po...</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>133</td>\n",
       "      <td>By 2009, the forecasts for both the headline a...</td>\n",
       "      <td>2022</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>297</td>\n",
       "      <td>In Japan, private consumption rebounded strong...</td>\n",
       "      <td>2006</td>\n",
       "      <td>2</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           sentence  year  label  \\\n",
       "0    687  Setting the horizon on the interest rate caps ...  2006      2   \n",
       "1    595  Nonetheless, employment is still 9.5 million b...  1999      0   \n",
       "2    824  The shifting balance of domestic demand and po...  2017      1   \n",
       "3    133  By 2009, the forecasts for both the headline a...  2022      2   \n",
       "4    297  In Japan, private consumption rebounded strong...  2006      2   \n",
       "\n",
       "   orig_index  \n",
       "0         666  \n",
       "1         576  \n",
       "2         801  \n",
       "3         130  \n",
       "4         283  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 78516\n",
    "df = pd.read_excel(f'../data/test/lab-manual-split-combine-test-{seed}.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_excel(f'../data/train/lab-manual-split-combine-train-{seed}.xlsx')\n",
    "sample = train.groupby('label', group_keys=False).apply(lambda x: x.sample(min(len(x), 10))).reset_index(drop=True)\n",
    "training_set = [dspy.Example(sentence = sample.loc[i].sentence, answer = sample.loc[i].label).with_inputs(\"sentence\") for i in range(len(sample.index))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_set = [dspy.Example(sentence = df.loc[i].sentence, answer = df.loc[i].label).with_inputs(\"sentence\") for i in range(len(df))]\n",
    "testing_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the horizon on the interest rate caps to reinforce forward guidance on the policy rate would augment the credibility of the yield curve caps and thereby diminish concerns about an open-ended balance sheet commitment.\n"
     ]
    }
   ],
   "source": [
    "sent = df.loc[0].sentence\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "turbo = dspy.OpenAI(model='gpt-3.5-turbo-instruct', max_tokens=1000, api_key=api_key)\n",
    "dspy.settings.configure(lm=turbo)\n",
    "d = {0: 'dovish', 1: 'hawkish', 2: 'neutral'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StanceAnalysis(dspy.Signature):\n",
    "    \"\"\"Classify the sentence's stance on the monetary policy between hawkish, neutral, and dovish.\"\"\"\n",
    "    \n",
    "    sentence = dspy.InputField()\n",
    "    stance = dspy.OutputField(desc = \"hawkish, neutral, or dovish\")\n",
    "\n",
    "class Analysis(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.predict = dspy.Predict(StanceAnalysis)\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        return self.predict(sentence=sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    stance='hawkish'\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify = dspy.Predict(StanceAnalysis)\n",
    "classify(sentence=sent)\n",
    "\n",
    "analyze = Analysis()\n",
    "analyze(df.loc[4].sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_match(example, pred, trace=None):\n",
    "    answer_match = d[example.answer].lower() == pred.stance.lower()\n",
    "    # print(f\"Actual: {d[example.answer].lower()}, Predicted: {pred.stance.lower()}\")\n",
    "    return answer_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 0 / 1  (0.0):   3%|▎         | 1/30 [00:00<00:09,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: dovish, Predicted: neutral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 2  (50.0):   7%|▋         | 2/30 [00:00<00:10,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: dovish, Predicted: dovish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 3  (33.3):  10%|█         | 3/30 [00:00<00:08,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: dovish, Predicted: neutral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 4  (50.0):  13%|█▎        | 4/30 [00:01<00:06,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: dovish, Predicted: dovish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 5  (60.0):  17%|█▋        | 5/30 [00:01<00:07,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: dovish, Predicted: dovish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4 / 6  (66.7):  20%|██        | 6/30 [00:02<00:12,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: dovish, Predicted: dovish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4 / 7  (57.1):  23%|██▎       | 7/30 [00:02<00:10,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: dovish, Predicted: neutral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4 / 8  (50.0):  27%|██▋       | 8/30 [00:03<00:09,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: dovish, Predicted: neutral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5 / 9  (55.6):  30%|███       | 9/30 [00:03<00:07,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: dovish, Predicted: dovish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 6 / 10  (60.0):  33%|███▎      | 10/30 [00:03<00:06,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: dovish, Predicted: dovish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 7 / 11  (63.6):  37%|███▋      | 11/30 [00:04<00:06,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: hawkish, Predicted: hawkish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8 / 12  (66.7):  40%|████      | 12/30 [00:04<00:05,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: hawkish, Predicted: hawkish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 9 / 13  (69.2):  43%|████▎     | 13/30 [00:04<00:04,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: hawkish, Predicted: hawkish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10 / 14  (71.4):  47%|████▋     | 14/30 [00:04<00:04,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: hawkish, Predicted: hawkish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 10 / 15  (66.7):  50%|█████     | 15/30 [00:04<00:03,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: hawkish, Predicted: dovish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11 / 16  (68.8):  53%|█████▎    | 16/30 [00:05<00:03,  4.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: hawkish, Predicted: hawkish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11 / 17  (64.7):  57%|█████▋    | 17/30 [00:05<00:03,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: hawkish, Predicted: neutral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11 / 18  (61.1):  60%|██████    | 18/30 [00:05<00:03,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: hawkish, Predicted: neutral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 12 / 19  (63.2):  63%|██████▎   | 19/30 [00:06<00:03,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: hawkish, Predicted: hawkish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 13 / 20  (65.0):  67%|██████▋   | 20/30 [00:06<00:02,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: hawkish, Predicted: hawkish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 14 / 21  (66.7):  70%|███████   | 21/30 [00:06<00:02,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: neutral, Predicted: neutral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 15 / 22  (68.2):  73%|███████▎  | 22/30 [00:06<00:02,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: neutral, Predicted: neutral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16 / 23  (69.6):  77%|███████▋  | 23/30 [00:07<00:02,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: neutral, Predicted: neutral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 17 / 24  (70.8):  80%|████████  | 24/30 [00:07<00:02,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: neutral, Predicted: neutral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 18 / 25  (72.0):  83%|████████▎ | 25/30 [00:07<00:01,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: neutral, Predicted: neutral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 18 / 26  (69.2):  87%|████████▋ | 26/30 [00:08<00:01,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: neutral, Predicted: hawkish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 19 / 27  (70.4):  90%|█████████ | 27/30 [00:08<00:00,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: neutral, Predicted: neutral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 20 / 28  (71.4):  93%|█████████▎| 28/30 [00:08<00:00,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: neutral, Predicted: neutral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 21 / 29  (72.4):  97%|█████████▋| 29/30 [00:08<00:00,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: neutral, Predicted: neutral\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 21 / 30  (70.0): 100%|██████████| 30/30 [00:09<00:00,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: neutral, Predicted: dovish\n",
      "Average Metric: 21 / 30  (70.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "70.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate = Evaluate(devset=training_set, metric=answer_match, display_progress=True, display_table=0)\n",
    "evaluate(Analysis())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\u001b[1mWARNING: Projected Language Model (LM) Calls\u001b[0m\n",
      "\n",
      "Please be advised that based on the parameters you have set, the maximum number of LM calls is projected as follows:\n",
      "\n",
      "\u001b[93m- Task Model: \u001b[94m\u001b[1m30\u001b[0m\u001b[93m examples in dev set * \u001b[94m\u001b[1m5\u001b[0m\u001b[93m trials * \u001b[94m\u001b[1m# of LM calls in your program\u001b[0m\u001b[93m = (\u001b[94m\u001b[1m150 * # of LM calls in your program\u001b[0m\u001b[93m) task model calls\u001b[0m\n",
      "\u001b[93m- Prompt Model: # data summarizer calls (max \u001b[94m\u001b[1m10\u001b[0m\u001b[93m) + \u001b[94m\u001b[1m10\u001b[0m\u001b[93m * \u001b[94m\u001b[1m1\u001b[0m\u001b[93m lm calls in program = \u001b[94m\u001b[1m20\u001b[0m\u001b[93m prompt model calls\u001b[0m\n",
      "\n",
      "\u001b[93m\u001b[1mEstimated Cost Calculation:\u001b[0m\n",
      "\n",
      "\u001b[93mTotal Cost = (Number of calls to task model * (Avg Input Token Length per Call * Task Model Price per Input Token + Avg Output Token Length per Call * Task Model Price per Output Token) \n",
      "            + (Number of calls to prompt model * (Avg Input Token Length per Call * Task Prompt Price per Input Token + Avg Output Token Length per Call * Prompt Model Price per Output Token).\u001b[0m\n",
      "\n",
      "For a preliminary estimate of potential costs, we recommend you perform your own calculations based on the task\n",
      "and prompt models you intend to use. If the projected costs exceed your budget or expectations, you may consider:\n",
      "\n",
      "\u001b[93m- Reducing the number of trials (`num_trials`), the size of the trainset, or the number of LM calls in your program.\u001b[0m\n",
      "\u001b[93m- Using a cheaper task model to optimize the prompt.\u001b[0m\n",
      "To proceed with the execution of this program, please confirm by typing \u001b[94m'y'\u001b[0m for yes or \u001b[94m'n'\u001b[0m for no.\n",
      "\n",
      "If you would like to bypass this confirmation step in future executions, set the \u001b[93m`requires_permission_to_run`\u001b[0m flag to \u001b[93m`False`.\u001b[0m\n",
      "\n",
      "\u001b[93mAwaiting your input...\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:01<00:11,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:00<00:06,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [00:00<00:05,  4.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [00:01<00:08,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:00<00:02, 11.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:00<00:03,  8.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [00:00<00:06,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [00:00<00:01, 14.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 6 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:00<00:02, 12.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-07 15:23:47,496] A new study created in memory with name: no-name-117fdb80-015e-464d-90b1-77fc31e966a4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 16 / 30  (53.3): 100%|██████████| 30/30 [00:07<00:00,  3.81it/s]\n",
      "[I 2024-04-07 15:23:55,374] Trial 0 finished with value: 53.33 and parameters: {'2170764072224_predictor_instruction': 1, '2170764072224_predictor_demos': 1}. Best is trial 0 with value: 53.33.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 16 / 30  (53.3%)\n",
      "Starting trial #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 17 / 30  (56.7): 100%|██████████| 30/30 [00:08<00:00,  3.35it/s]\n",
      "[I 2024-04-07 15:24:04,325] Trial 1 finished with value: 56.67 and parameters: {'2170764072224_predictor_instruction': 5, '2170764072224_predictor_demos': 4}. Best is trial 1 with value: 56.67.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 17 / 30  (56.7%)\n",
      "Starting trial #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11 / 30  (36.7): 100%|██████████| 30/30 [00:09<00:00,  3.21it/s]\n",
      "[I 2024-04-07 15:24:13,670] Trial 2 finished with value: 36.67 and parameters: {'2170764072224_predictor_instruction': 3, '2170764072224_predictor_demos': 0}. Best is trial 1 with value: 56.67.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 11 / 30  (36.7%)\n",
      "Starting trial #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 19 / 30  (63.3): 100%|██████████| 30/30 [00:09<00:00,  3.18it/s]\n",
      "[I 2024-04-07 15:24:23,116] Trial 3 finished with value: 63.33 and parameters: {'2170764072224_predictor_instruction': 9, '2170764072224_predictor_demos': 3}. Best is trial 3 with value: 63.33.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19 / 30  (63.3%)\n",
      "Starting trial #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 19 / 30  (63.3): 100%|██████████| 30/30 [00:09<00:00,  3.33it/s]\n",
      "[I 2024-04-07 15:24:32,130] Trial 4 finished with value: 63.33 and parameters: {'2170764072224_predictor_instruction': 8, '2170764072224_predictor_demos': 4}. Best is trial 3 with value: 63.33.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 19 / 30  (63.3%)\n",
      "Returning predict = Predict(StringSignature(sentence -> stance\n",
      "    instructions=\"classify the sentence's stance on the monetary policy between supportive, neutral, and opposing.\"\n",
      "    sentence = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Sentence:', 'desc': '${sentence}'})\n",
      "    stance = Field(annotation=str required=True json_schema_extra={'desc': 'hawkish, neutral, or dovish', '__dspy_field_type': 'output', 'prefix': '[Classification]\", followed by the suggested stance for the prompt, for example: \"[Classification] hawkish\", \"[Classification] neutral\", or \"[Classification] dovish'})\n",
      ")) from continue_program\n"
     ]
    }
   ],
   "source": [
    "teleprompter = MIPRO(metric = answer_match)\n",
    "optimized_program = teleprompter.compile(Analysis(), trainset = training_set, num_trials=5, max_bootstrapped_demos=3, max_labeled_demos=3, eval_kwargs=dict(display_progress=True, display_table=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict = Predict(StringSignature(sentence -> stance\n",
       "    instructions=\"classify the sentence's stance on the monetary policy between supportive, neutral, and opposing.\"\n",
       "    sentence = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Sentence:', 'desc': '${sentence}'})\n",
       "    stance = Field(annotation=str required=True json_schema_extra={'desc': 'hawkish, neutral, or dovish', '__dspy_field_type': 'output', 'prefix': '[Classification]\", followed by the suggested stance for the prompt, for example: \"[Classification] hawkish\", \"[Classification] neutral\", or \"[Classification] dovish'})\n",
       "))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 323 / 496  (65.1): 100%|██████████| 496/496 [02:32<00:00,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 323 / 496  (65.1%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate = Evaluate(devset=testing_set, metric=answer_match, display_progress=True, return_outputs=True)\n",
    "outputs = evaluate(optimized_program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6512096774193549\n",
      "F1: 0.6398555274413625\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Setting the horizon on the interest rate caps ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nonetheless, employment is still 9.5 million b...</td>\n",
       "      <td>dovish</td>\n",
       "      <td>dovish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The shifting balance of domestic demand and po...</td>\n",
       "      <td>hawkish</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>By 2009, the forecasts for both the headline a...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In Japan, private consumption rebounded strong...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence   Actual Predicted\n",
       "0  Setting the horizon on the interest rate caps ...  neutral   neutral\n",
       "1  Nonetheless, employment is still 9.5 million b...   dovish    dovish\n",
       "2  The shifting balance of domestic demand and po...  hawkish   neutral\n",
       "3  By 2009, the forecasts for both the headline a...  neutral   neutral\n",
       "4  In Japan, private consumption rebounded strong...  neutral   neutral"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outdf = pd.DataFrame(columns=['Sentence', 'Actual', 'Predicted'])\n",
    "for pred in outputs[1]:\n",
    "    outdf.loc[len(outdf)] = [pred[0].sentence, d[pred[0].answer].lower(), pred[1].stance.lower()]\n",
    "outdf.to_csv(f\"../data/llm_prompt_outputs/dspy_{seed}.csv\", index=False)\n",
    "print(f\"Accuracy: {accuracy_score(outdf['Actual'], outdf['Predicted'])}\")\n",
    "print(f\"F1: {f1_score(outdf['Actual'], outdf['Predicted'], average='weighted')}\")\n",
    "outdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6411790980666666\n",
      "0.028620621062180572\n"
     ]
    }
   ],
   "source": [
    "print(np.mean([0.6068066709, 0.6398555274, 0.6768750959]))\n",
    "print(np.std([0.6068066709, 0.6398555274, 0.6768750959]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
